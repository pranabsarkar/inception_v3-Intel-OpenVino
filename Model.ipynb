{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force use CPU only.\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 as Net\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img_height = 224\n",
    "\n",
    "model = Net(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02504458', 'African_elephant', 0.9775383), ('n02504013', 'Indian_elephant', 0.017116355), ('n01871265', 'tusker', 0.005325458)]\n"
     ]
    }
   ],
   "source": [
    "# Optional image to test model prediction.\n",
    "img_path = 'data/elephant.jpg'\n",
    "\n",
    "# Path to save the model h5 file.\n",
    "model_fname = 'model/model.h5'\n",
    "\n",
    "# Load the image for prediction.\n",
    "img = image.load_img(img_path, target_size=(img_height, img_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]\n",
    "\n",
    "# Save the h5 file to path specified.\n",
    "model.save(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "['input_2'] ['predictions/Softmax']\n",
      "INFO:tensorflow:Froze 378 variables.\n",
      "INFO:tensorflow:Converted 378 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Clear any previous session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "save_pb_dir = 'model/'\n",
    "model_fname = 'model/model.h5'\n",
    "def freeze_graph(graph, session, output, save_pb_dir='.', save_pb_name='frozen_model.pb', save_pb_as_text=False):\n",
    "    with graph.as_default():\n",
    "        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
    "        graphdef_frozen = tf.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
    "        graph_io.write_graph(graphdef_frozen, save_pb_dir, save_pb_name, as_text=save_pb_as_text)\n",
    "        return graphdef_frozen\n",
    "\n",
    "# This line must be executed before loading Keras model.\n",
    "tf.keras.backend.set_learning_phase(0) \n",
    "\n",
    "model = load_model(model_fname)\n",
    "\n",
    "session = tf.keras.backend.get_session()\n",
    "\n",
    "INPUT_NODE = [t.op.name for t in model.inputs]\n",
    "OUTPUT_NODE = [t.op.name for t in model.outputs]\n",
    "print(INPUT_NODE, OUTPUT_NODE)\n",
    "frozen_graph = freeze_graph(session.graph, session, [out.op.name for out in model.outputs], save_pb_dir=save_pb_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1,224,224,3]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# force reset ipython namespaces\n",
    "%reset -f\n",
    "\n",
    "import platform\n",
    "is_win = 'windows' in platform.platform().lower()\n",
    "\n",
    "# OpenVINO 2019\n",
    "if is_win:\n",
    "    mo_tf_path = '\"C:\\Program Files (x86)\\IntelSWTools\\openvino\\deployment_tools\\model_optimizer\\mo_tf.py\"'\n",
    "else:\n",
    "    # mo_tf.py path in Linux\n",
    "    mo_tf_path = '/opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py'\n",
    "\n",
    "pb_file = 'model/frozen_model.pb'\n",
    "output_dir = 'model/'\n",
    "img_height = 224\n",
    "input_shape = [1, img_height, img_height, 3]\n",
    "input_shape_str = str(input_shape).replace(' ', '')\n",
    "input_shape_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tC:\\Users\\sarka\\Desktop\\OpenVino\\model/frozen_model.pb\n",
      "\t- Path for generated IR: \tC:\\Users\\sarka\\Desktop\\OpenVino\\model/\n",
      "\t- IR output name: \tfrozen_model\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,224,224,3]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2019.1.1-83-g28dfbfd\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: C:\\Users\\sarka\\Desktop\\OpenVino\\model/frozen_model.xml\n",
      "[ SUCCESS ] BIN file: C:\\Users\\sarka\\Desktop\\OpenVino\\model/frozen_model.bin\n",
      "[ SUCCESS ] Total execution time: 179.20 seconds. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarka\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "!python {mo_tf_path} --input_model {pb_file} --output_dir {output_dir} --input_shape {input_shape_str} --data_type FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following error happened while importing Python API module:\n",
      "[ ModuleNotFoundError ] No module named 'openvino'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4e18d9d9d33d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mopenvino\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minference_engine\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mie\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mopenvino\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_engine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIENetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIEPlugin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openvino'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4e18d9d9d33d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mexception_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The following error happened while importing Python API module:\\n[ {} ] {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_process_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "try:\n",
    "    from openvino import inference_engine as ie\n",
    "    from openvino.inference_engine import IENetwork, IEPlugin\n",
    "except Exception as e:\n",
    "    exception_type = type(e).__name__\n",
    "    print(\"The following error happened while importing Python API module:\\n[ {} ] {}\".format(exception_type, e))\n",
    "    sys.exit(1)\n",
    "\n",
    "def pre_process_image(imagePath, img_height=224):\n",
    "    # Model input format\n",
    "    n, c, h, w = [1, 3, img_height, img_height]\n",
    "    image = Image.open(imagePath)\n",
    "    processedImg = image.resize((h, w), resample=Image.BILINEAR)\n",
    "\n",
    "    # Normalize to keep data between 0 - 1\n",
    "    processedImg = (np.array(processedImg) - 0) / 255.0\n",
    "\n",
    "    # Change data layout from HWC to CHW\n",
    "    processedImg = processedImg.transpose((2, 0, 1))\n",
    "    processedImg = processedImg.reshape((n, c, h, w))\n",
    "\n",
    "    return image, processedImg, imagePath\n",
    "\n",
    "# Plugin initialization for specified device and load extensions library if specified.\n",
    "plugin_dir = None\n",
    "model_xml = 'model/frozen_model.xml'\n",
    "model_bin = 'model/frozen_model.bin'\n",
    "# Devices: GPU (intel), CPU, MYRIAD\n",
    "plugin = IEPlugin(\"CPU\", plugin_dirs=plugin_dir)\n",
    "# Read IR\n",
    "net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n",
    "assert len(net.inputs.keys()) == 1\n",
    "assert len(net.outputs) == 1\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "# Load network to the plugin\n",
    "exec_net = plugin.load(network=net)\n",
    "del net\n",
    "\n",
    "# Run inference\n",
    "fileName = 'data/elephant.jpg'\n",
    "image, processedImg, imagePath = pre_process_image(fileName)\n",
    "res = exec_net.infer(inputs={input_blob: processedImg})\n",
    "# Access the results and get the index of the highest confidence score\n",
    "output_node_name = list(res.keys())[0]\n",
    "res = res[output_node_name]\n",
    "\n",
    "# Predicted class index.\n",
    "idx = np.argsort(res[0])[-1]\n",
    "\n",
    "# decode the predictions\n",
    "from tensorflow.keras.applications.inception_v3 import decode_predictions\n",
    "print('Predicted:', decode_predictions(res, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
